{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Td(0) Prediction\n",
    "\n",
    "Here, I have used TD(0) Prediction algorithm to predict the value of the states for the uniform random policy of selecting each action ('left', 'right', 'up', and 'down') with 0.25 probability given a state s. The Td(0) Prediction algorithm is run for 100 episodes during which the value of the states are updated. After, this 100 more episodes are simulated. The squared error at each episode (of these extra 100 simualted episodes) is found using $SE_t = (V_{\\pi}(s) - V(s))^2$ where $V_{\\pi}(s)$ is the true value of the states calculated using policy evaluation and V(s) is the TD estimate of the states. We sum these squares for all states and take the average to have the mean squared error for each extra episodes simulated from 101 to 200. The average of these mean squared errors over these 100 simulated episodes gives us the required mean squared error. Given a value of the step size $\\alpha$, this process is repeated to find the mean squared value error for the given alpha. The plot shows the behaviour of the mean squarred error across different values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the gridworld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the environment\n",
    "class Environment:\n",
    "    def __init__(self, M,N, holes, terminal_state): #initializing the environment states, holes, terminals and rewards\n",
    "        self.states = set()\n",
    "        self.shape = (M,N)\n",
    "        self.holes = holes\n",
    "        self.terminal_state = terminal_state\n",
    "        \n",
    "        for i in range(1,M+1):\n",
    "            for j in range(1, N+1):\n",
    "                if (i,j) not in holes:\n",
    "                    self.states.add((i,j))\n",
    "                    \n",
    "        self.rewards = self.initialize_rewards()\n",
    "        self.prob_agent_action = [0.8, 0.1, 0.05, 0.05]\n",
    "    def initialize_rewards(self): #function to initialize the rewards for each state of the environment\n",
    "        r = {}\n",
    "        for state in self.states:\n",
    "            if state == (6,3):\n",
    "                r[state] = -15\n",
    "            elif state == (6,6):\n",
    "                r[state] = 15\n",
    "            else:\n",
    "                r[state] = 0\n",
    "                \n",
    "        return r\n",
    "    \n",
    "    def agent_move(self, s, a): #function to update the state of the agent given an action a and current state s\n",
    "        x, y = s\n",
    "        if a=='U':\n",
    "            x = x-1\n",
    "        elif a=='D':\n",
    "            x = x + 1\n",
    "        elif a=='R':\n",
    "            y = y + 1\n",
    "        elif a=='L':\n",
    "            y = y - 1\n",
    "            \n",
    "        stay_same = self.check_corner_and_hole((x,y))\n",
    "\n",
    "        if stay_same:\n",
    "            return s\n",
    "\n",
    "        return (x,y)\n",
    "\n",
    "    def move_clockwise90(self, a): #function to return the action which is a 90 degree rotation to current action a\n",
    "        if a=='U':\n",
    "            return 'R'\n",
    "        elif a=='R':\n",
    "            return 'D'\n",
    "        elif a=='D':\n",
    "            return 'L'\n",
    "        elif a=='L':\n",
    "            return 'U'\n",
    "        \n",
    "    def move_anti_clockwise90(self, a): #function to return the action which is a 90 degree rotation to current action a\n",
    "        if a=='U':\n",
    "            return 'L'\n",
    "        elif a=='L':\n",
    "            return 'D'\n",
    "        elif a=='D':\n",
    "            return 'R'\n",
    "        elif a=='R':\n",
    "            return 'U'\n",
    "    \n",
    "    def check_corner_and_hole(self, s): \n",
    "        #function to check if the updates state goes out of the gridworld or goes into holes. \n",
    "        #If so, it returns a True value to address that the update should not take place and agent should remain in current state. \n",
    "        x1, y1 = s\n",
    "        stay_same = False\n",
    "        for hole in self.holes:\n",
    "            if (x1,y1) == hole:\n",
    "                stay_same = True\n",
    "\n",
    "        if x1<1 or x1>6:\n",
    "            stay_same = True\n",
    "        if y1<1 or y1>6:\n",
    "            stay_same = True\n",
    "        \n",
    "        return stay_same\n",
    "    \n",
    "    def get_new_state(self, s, a): \n",
    "        #this is the function to take the agent to an update state given the agent's choice of action and current state.\n",
    "        #This encapsulates the dynamics of the environment and is not known to the agent. The agent only\n",
    "        #produces the current state s and his choice of action a, to which the environment returns his new state s1.\n",
    "        x, y = s\n",
    "        t = random.random()\n",
    "        \n",
    "        if t<=0.8: #agent's action succeeds\n",
    "            x1,y1 = self.agent_move(s,a)\n",
    "            return (x1,y1)\n",
    "        \n",
    "        elif 0.8<t<=0.9: #agent stays in same state\n",
    "            return s\n",
    "        \n",
    "        elif 0.9<t<=0.95: #move in a 90 degree clockwise direction\n",
    "            a1 = self.move_clockwise90(a)\n",
    "            s1 = self.agent_move(s, a1)\n",
    "            return s1\n",
    "        else: #move in a -90 degree clockwise direction\n",
    "            a1 = self.move_anti_clockwise90(a)\n",
    "            s1 = self.agent_move(s, a1)\n",
    "            \n",
    "            return s1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld = Environment(6,6 , [(4,3),(5,3)], (6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the value of states using Policy Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value(env = gridworld, epsilon = 1e-8, gamma = 0.9):\n",
    "    #function for policy evaluation to compute value of states for the uniform random policy\n",
    "    delta = np.inf\n",
    "    V = {}\n",
    "    actions = ['L','R','U','D']\n",
    "    pi = {}\n",
    "    \n",
    "    #initializing the value of all states and the uniform random policy of taking each action with prob 0.25\n",
    "    for s in env.states:\n",
    "        V[s] = 0\n",
    "        pi[s] = {}\n",
    "        for a in actions:\n",
    "            pi[s][a] = 0.25\n",
    "\n",
    "    #loop forever till abs(new value - old value) is withing epsilon\n",
    "    while delta > epsilon:\n",
    "        max_diff = -np.inf\n",
    "        for s in env.states:\n",
    "            if s!=env.terminal_state:\n",
    "                v = 0\n",
    "                for a in pi[s]:\n",
    "                    p = pi[s][a]\n",
    "                    #with prob 0.8 take action a to get to state s1 with reward r1\n",
    "                    s1 = env.agent_move(s,a)\n",
    "                    r1 = env.rewards[s1]\n",
    "\n",
    "                    v1 = 0.8 * (r1 + gamma * V[s1])\n",
    "\n",
    "                    #with prob 0.1 stay in same state s\n",
    "                    r2 = env.rewards[s]\n",
    "                    v2 = 0.1 * (r2 + gamma * V[s])\n",
    "\n",
    "                    #with prob 0.05 take action in direction +90 degree clockwise direction\n",
    "                    a3 = env.move_clockwise90(a)\n",
    "                    s3 = env.agent_move(s, a3)\n",
    "                    r3 = env.rewards[s3]\n",
    "                    v3 = 0.05 * (r3 + gamma * V[s3])\n",
    "\n",
    "                    #with prob 0.05 take action in direction -90 degree clockwise direction\n",
    "\n",
    "                    a4 = env.move_anti_clockwise90(a)\n",
    "                    s4 = env.agent_move(s, a4)\n",
    "                    r4 = env.rewards[s4]\n",
    "                    v4 = 0.05 * (r4 + gamma * V[s4])\n",
    "\n",
    "                    v = v + (p * (v1+v2+v3+v4))\n",
    "                    \n",
    "\n",
    "                diff = abs(V[s] - v)\n",
    "                V[s] = v\n",
    "                max_diff = max(max_diff, diff)\n",
    "\n",
    "        delta = max_diff\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 3): -0.27642130303408846,\n",
       " (6, 6): 0,\n",
       " (5, 6): 5.428752086365931,\n",
       " (2, 1): -0.7682316919867802,\n",
       " (6, 2): -18.202778672087756,\n",
       " (1, 6): 0.0674715652078087,\n",
       " (5, 1): -5.954288579506516,\n",
       " (2, 5): 0.005714819711983525,\n",
       " (1, 2): -0.40649480892936374,\n",
       " (3, 3): -0.6621033994813392,\n",
       " (5, 5): 0.5071660442594326,\n",
       " (4, 4): -1.2840003336659664,\n",
       " (6, 3): -29.301131575796326,\n",
       " (1, 5): -0.022069683166935392,\n",
       " (3, 6): 0.5918009837333996,\n",
       " (2, 2): -0.6727476405258225,\n",
       " (4, 1): -3.173690884069524,\n",
       " (1, 1): -0.47105368791522395,\n",
       " (6, 4): -14.12843836659732,\n",
       " (3, 2): -1.4395057149396342,\n",
       " (2, 6): 0.1903320760526663,\n",
       " (5, 4): -4.266173444005633,\n",
       " (4, 5): 0.24488791723038597,\n",
       " (5, 2): -7.942819537533008,\n",
       " (1, 4): -0.1502940589464806,\n",
       " (2, 3): -0.40897942059492143,\n",
       " (4, 2): -3.5937713998598255,\n",
       " (6, 5): 0.8716499724036371,\n",
       " (3, 5): 0.08402948769442829,\n",
       " (4, 6): 1.7932887647332514,\n",
       " (6, 1): -9.6867447798331,\n",
       " (3, 1): -1.5402674582274198,\n",
       " (3, 4): -0.4647897341603413,\n",
       " (2, 4): -0.2266104937223239}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V contains the value of states evaluated using policy evaluation and is the true state value for the uniform random policy.\n",
    "# This value is going to be compared with the TD estimate to compute the mean squared error\n",
    "V = compute_value()\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD(0) Prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the TD Agent for TD(0) prediction for the uniform random policy of selecting each action with probability 0.25\n",
    "class TD_Agent:\n",
    "    def __init__(self, alpha, gamma, env):\n",
    "        #initializing the parameters of the agent\n",
    "        self.actions = ['L','R','U','D'] #possible actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma #discount parameter\n",
    "        self.V = self.initialize_value_states(env)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def initialize_value_states(self, env):\n",
    "        #function to initialize the value of the states\n",
    "        v_s = {}\n",
    "        for state in env.states:\n",
    "            v_s[state] = 0\n",
    "        return v_s\n",
    "    \n",
    "    \n",
    "    def TD_update(self, env, start_state = (1,1)):\n",
    "        #function to update the value of states\n",
    "        s = start_state\n",
    "        while s!= env.terminal_state:\n",
    "            #all actions are possible with uniform proability \n",
    "            a = np.random.choice(self.actions)\n",
    "            s1 = env.get_new_state(s,a)\n",
    "            r = env.rewards[s1]\n",
    "            \n",
    "            delta_t = (r + (self.gamma*self.V[s1])) - self.V[s]\n",
    "            self.V[s] = self.V[s] + (self.alpha * delta_t)\n",
    "            \n",
    "            s = s1\n",
    "        \n",
    "    def play(self, env, episodes = 100):\n",
    "        #function to play the TD agent for 100 episodes\n",
    "        for episode_no in range(episodes):\n",
    "            self.TD_update(env)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " def find_mse_value_states(agent, V, env = gridworld,start_state = (1,1), episodes = 100):\n",
    "        #function to find the mean squared TD error of each state. \n",
    "        \n",
    "        #td_errors = []\n",
    "        t = 0\n",
    "        td_error_sum = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(episodes):\n",
    "            s = start_state\n",
    "            v_ep = agent.V.copy()\n",
    "            \n",
    "            while s!=env.terminal_state:\n",
    "                t+=1\n",
    "                a = np.random.choice(agent.actions, p = [0.25, 0.25, 0.25, 0.25])\n",
    "                s1 = env.get_new_state(s,a)\n",
    "                r = env.rewards[s1]\n",
    "\n",
    "                v_ep[s] = v_ep[s] + (agent.alpha*(r + (agent.gamma*v_ep[s1]) - v_ep[s]))\n",
    "                #td_error_s = V[s] - agent.V[s] \n",
    "                #td_error_s = r + (agent.gamma* agent.V[s1]) - agent.V[s]\n",
    "                #td_error_sum += (td_error_s**2)\n",
    "                s = s1\n",
    "                \n",
    "            mse_ep = 0\n",
    "            for s in env.states:\n",
    "                error = V[s] - v_ep[s]\n",
    "                mse_ep += (error**2)\n",
    "                \n",
    "            mse_ep/=len(env.states)\n",
    "            td_error_sum +=mse_ep\n",
    "\n",
    "            #mse = td_error_sum/n\n",
    "            #td_errors.append(mse)    \n",
    "        \n",
    "        return (td_error_sum/episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem1():\n",
    "    alphas = [0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "    #alphas = np.linspace(0.001, 0.1, 100)\n",
    "    mean_squared_td_errors = []\n",
    "    for alpha in tqdm(alphas):\n",
    "        agent = TD_Agent(alpha = alpha, gamma = 0.9, env = gridworld)\n",
    "        agent.play(gridworld)\n",
    "        mse_td_error = find_mse_value_states(agent, V)\n",
    "        mean_squared_td_errors.append(mse_td_error)\n",
    "    \n",
    "    print (\"alpha\\tMSE\")\n",
    "    for i in range(len(alphas)):\n",
    "        print (alphas[i], mean_squared_td_errors[i])\n",
    "        \n",
    "    plt.plot(alphas, mean_squared_td_errors)\n",
    "    plt.xlabel(\"Step Size\")\n",
    "    plt.ylabel(\"Mean squared Td error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha\tMSE\n",
      "0.0001 48.16592267888045\n",
      "0.0005 43.799154455525404\n",
      "0.001 38.47126810969493\n",
      "0.002 35.762218763835634\n",
      "0.005 19.8801459565938\n",
      "0.01 11.034999779449283\n",
      "0.02 2.3004128908537127\n",
      "0.05 0.7810092986815761\n",
      "0.1 3.0720831551284213\n",
      "0.2 5.080750365059172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "problem1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4020074711510651"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = TD_Agent(alpha = 0.05, gamma = 0.9, env = gridworld)\n",
    "agent.play(gridworld)\n",
    "    \n",
    "agent.V[(1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
