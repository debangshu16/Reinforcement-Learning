{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfwOBk4jV-eQ"
      },
      "source": [
        "**Importing necessary packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_JFJJSFOLI5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.colors import ListedColormap\r\n",
        "import itertools\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9JASDAWN0H"
      },
      "source": [
        "**Building Environment class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6GvhBcXOmi_"
      },
      "source": [
        "#This class is used to represent the race track as the environment\r\n",
        "\r\n",
        "class Environment:\r\n",
        "  def __init__(self, track_file, reward=10):\r\n",
        "    #Initializing the environment by reading the race track file\r\n",
        "    with open(track_file,'r') as f:\r\n",
        "      track = f.readlines()\r\n",
        "\r\n",
        "    #Building the environment\r\n",
        "    self.arr = self.build_track(track)\r\n",
        "    self.m, self.n = self.arr.shape[0], self.arr.shape[1]\r\n",
        "    #Initializing the reward for going out of track, collision or reaching goal\r\n",
        "    self.reward = reward\r\n",
        "\r\n",
        "\r\n",
        "  def build_track(self, track):      #Helper function to build the racetrack as environment\r\n",
        "    ''' Mapping : \r\n",
        "    0: Start State (S)\r\n",
        "    1: Other State (O)\r\n",
        "    2: Obstacles State(X)\r\n",
        "    3: Finish State(F) \r\n",
        "    '''\r\n",
        "    if track[-1]=='\\n':\r\n",
        "        track.pop()\r\n",
        "      \r\n",
        "    m = len(track)\r\n",
        "      \r\n",
        "    arr = []\r\n",
        "\r\n",
        "    for i in range(m-1,-1,-1):\r\n",
        "      t = []\r\n",
        "      for j in range(len(track[i])):\r\n",
        "        if track[i][j]!='\\n':\r\n",
        "          if track[i][j] =='S':\r\n",
        "            t.append(0)\r\n",
        "          elif track[i][j] =='O':\r\n",
        "            t.append(1)\r\n",
        "          elif track[i][j] == 'X':\r\n",
        "            t.append(2)\r\n",
        "          elif track[i][j] == 'F':\r\n",
        "            t.append(3)\r\n",
        "      arr.append(t)\r\n",
        "\r\n",
        "    arr = np.array(arr)\r\n",
        "    return arr\r\n",
        "  \r\n",
        "  def plot_track(self):                 #Helper function to plot the race track\r\n",
        "    cmap = ListedColormap(['r','gray','k','g'])\r\n",
        "    \r\n",
        "    t = np.zeros((self.m, self.n))\r\n",
        "    for i in range(self.m-1, -1, -1):\r\n",
        "      for j in range(self.n):\r\n",
        "        t[(self.m-1) - i][j] = self.arr[i][j]\r\n",
        "\r\n",
        "    plt.matshow(t, cmap = cmap)\r\n",
        "    #plt.legend()\r\n",
        "    plt.axis('off')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "  def plot_episode(self,episode, end_state):      #Helper function to plot an episode of the agent(car)\r\n",
        "\r\n",
        "    t1 = self.arr.copy()\r\n",
        "    for ep in episode:\r\n",
        "      s = ep[0]\r\n",
        "      i,j = s\r\n",
        "      t1[i,j] = 4\r\n",
        "\r\n",
        "    if (end_state[0]>=(self.m-1)) and (0<=end_state[1]<self.n):\r\n",
        "      t1[self.m-1,end_state[1]] = 4 \r\n",
        "    elif (0<=end_state[0]<self.m) and (0<=end_state[1]<self.n):\r\n",
        "      t1[end_state[0], end_state[1]] = 4\r\n",
        "    \r\n",
        "    cmap = ListedColormap(['r','gray','k','g','b'])\r\n",
        "    \r\n",
        "    t = np.zeros((self.m, self.n))\r\n",
        "    for i in range(self.m-1, -1, -1):\r\n",
        "      for j in range(self.n):\r\n",
        "        t[(self.m-1) - i][j] = t1[i][j]\r\n",
        "\r\n",
        "    plt.imshow(t, cmap = cmap)\r\n",
        "    \r\n",
        "    plt.axis('off')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "  #Helper function to get the new state of the agent based on it's past state and action chosen \r\n",
        "  def update_state(self, s, a):      \r\n",
        "    s_new = (s[0] + a[0], s[1] + a[1])\r\n",
        "    return (s_new)\r\n",
        "\r\n",
        "  #Helper function to check if a given state is a terminal state\r\n",
        "  def isTerminal(self, s):            #\r\n",
        "    i = s[0]\r\n",
        "    j = s[1]\r\n",
        "    if (i<0) or (j<0) or (j>=self.n) or(i>=self.m):     #went out of racetrack\r\n",
        "      return True\r\n",
        "    if (self.arr[i,j]==2) or (self.arr[i,j]==3):        #hitting an obstacle or reaching goal\r\n",
        "      return True\r\n",
        "    \r\n",
        "    return False\r\n",
        "\r\n",
        "  #Helper function to get reward for current state\r\n",
        "  def get_reward(self,state, hit_obstacle=False):       \r\n",
        "    if hit_obstacle ==True:\r\n",
        "      return -(self.reward)\r\n",
        "    \r\n",
        "    i,j = state[0], state[1]\r\n",
        "    if (i<0) or (j<0) or (j>=self.n):\r\n",
        "      return -(self.reward)\r\n",
        "    elif (i>=self.m):\r\n",
        "      return self.reward\r\n",
        "    elif self.arr[i,j]==3:\r\n",
        "      return self.reward\r\n",
        "    elif self.arr[i,j]==2:\r\n",
        "      return -(self.reward)\r\n",
        "    \r\n",
        "    \r\n",
        "    return -1\r\n",
        "\r\n",
        "  #Helper function to sample a start state for the agent\r\n",
        "  def sample_start_state(self):\r\n",
        "    j = np.random.randint(0,self.n)\r\n",
        "    return (0,j)\r\n",
        "    \r\n",
        " "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJiw0JoOzUK"
      },
      "source": [
        "**Q Learning Method**\r\n",
        "\r\n",
        "For this problem, I am going to use the Q-Learning Algorithm. I am going to represent the the agent(car) by the TD_Agent class. The algorithm is implemented by the method Q_L."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IV-xtkdAWRt"
      },
      "source": [
        "#Class to represent the agent(car)\r\n",
        "class TD_Agent:\r\n",
        "  def __init__(self, epsilon = 0.2):\r\n",
        "    #Building the agent with parameters epsilon, the action space possible and initial velocity\r\n",
        "    self.epsilon = epsilon\r\n",
        "    self.actions = self.actions()\r\n",
        "    self.velocity = (0,0)\r\n",
        "\r\n",
        "\r\n",
        "  def initialize(self, states, env):\r\n",
        "\r\n",
        "    #Initialize  q(s,a) arbitrarily for each state and action except the target state which has q(s,a) = 0\r\n",
        "    \r\n",
        "    self.q = {}\r\n",
        "    \r\n",
        "    for s in states:\r\n",
        "      for a in self.actions:\r\n",
        "        s_a = (s,a)\r\n",
        "        \r\n",
        "        if (not env.isTerminal(s)):\r\n",
        "          self.q[s_a] = np.random.normal(-1,1)\r\n",
        "        else:\r\n",
        "          self.q[s_a] = 0                 \r\n",
        "        \r\n",
        "\r\n",
        "  # Helper function to get the possible actions for a given state  \r\n",
        "  def get_action(self,state, learn = True):\r\n",
        "    '''learn parameter is used to control if the action sampled should be with respect to if the agent is learning or if it is learnt.\r\n",
        "    If agent is learning policy is an epsilon greedy policy. Else, we take the greedy approach of chosing action with max(q(s,a)).\r\n",
        "    '''\r\n",
        "    if learn==True:\r\n",
        "      t = -1\r\n",
        "      prob_sample = np.random.uniform(0,1)\r\n",
        "      if prob_sample <= self.epsilon:     #to explore\r\n",
        "        while True:\r\n",
        "          #action = self.actions[np.random.randint(0, len(self.actions))]\r\n",
        "          action = random.choice(self.actions)\r\n",
        "          if self.action_valid(action):\r\n",
        "            a = action\r\n",
        "            break\r\n",
        "      \r\n",
        "      else:           #to exploit\r\n",
        "        t = -999 \r\n",
        "        best_actions = []\r\n",
        "        for action in self.actions:\r\n",
        "          x = (state, action)\r\n",
        "          \r\n",
        "          if (self.q[x]>t) and (self.action_valid(action)):\r\n",
        "            best_actions = []\r\n",
        "            t = self.q[x]\r\n",
        "            best_actions.append(action)\r\n",
        "          elif (self.q[x]==t) and (self.action_valid(action)):\r\n",
        "            t = self.q[x]\r\n",
        "            best_actions.append(action)\r\n",
        "\r\n",
        "        if len(best_actions)>1:\r\n",
        "          #t = np.random.randint(0, len(best_actions))\r\n",
        "          #a = best_actions[t]\r\n",
        "          a = random.choice(best_actions)\r\n",
        "        else:\r\n",
        "          a = best_actions[0]\r\n",
        "    \r\n",
        "    else:       #sample action based on learnt agent\r\n",
        "      t = -999 \r\n",
        "      best_actions = []\r\n",
        "      for action in self.actions:\r\n",
        "        x = (state, action)\r\n",
        "        \r\n",
        "        if (self.q[x]>t) and (self.action_valid(action)):\r\n",
        "          best_actions = []\r\n",
        "          t = self.q[x]\r\n",
        "          best_actions.append(action)\r\n",
        "        elif (self.q[x]==t) and (self.action_valid(action)):\r\n",
        "          #t = self.policy[x]\r\n",
        "          best_actions.append(action)\r\n",
        "\r\n",
        "      if len(best_actions)>1:\r\n",
        "        #t = np.random.randint(0, len(best_actions))\r\n",
        "        #a = best_actions[t]\r\n",
        "        a = random.choice(best_actions)\r\n",
        "      else:\r\n",
        "        a = best_actions[0]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    return a\r\n",
        "\r\n",
        "  #Helper function to check if action chosen is a valid action with respect to agent constraints\r\n",
        "  def action_valid(self, a):\r\n",
        "    vx = self.velocity[0] + a[0]\r\n",
        "    vy = self.velocity[1] + a[1]\r\n",
        "\r\n",
        "    if (vx==0 and vy==0):   #velocity cannot be zero except at start position\r\n",
        "      return False\r\n",
        "\r\n",
        "    if (np.abs(vx)<5) and (np.abs(vy)<5):       #absolute value of each velocity component must be less than 5\r\n",
        "      return True\r\n",
        "    \r\n",
        "    return False\r\n",
        "\r\n",
        "  #Helper function to update velocity of agent(car)\r\n",
        "  def update_velocity(self, a):\r\n",
        "    vx = self.velocity[0] + a[0]\r\n",
        "    vy = self.velocity[1] + a[1]\r\n",
        "    self.velocity = (vx, vy)\r\n",
        "\r\n",
        "  #Helper function to reset car velocity after end of episode\r\n",
        "  def reset_car(self):\r\n",
        "    self.velocity = (0,0)\r\n",
        "\r\n",
        "  #Helper function to chose greedily the action which gives max(q(s,a)) for a given state  \r\n",
        "  def get_a_star(self, s):\r\n",
        "    t = -999\r\n",
        "    best_actions = []\r\n",
        "    for a in self.actions:\r\n",
        "      state = (s,a)\r\n",
        "      if self.q[state] > t:\r\n",
        "        best_actions = []\r\n",
        "        t = self.q[state]\r\n",
        "        best_actions.append(a)\r\n",
        "      elif self.q[state]==t:\r\n",
        "        best_actions.append(a)\r\n",
        "    \r\n",
        "    if len(best_actions)>1:\r\n",
        "      a_star = random.choice(best_actions)\r\n",
        "    else:\r\n",
        "      a_star = best_actions[0]\r\n",
        "\r\n",
        "    return a_star\r\n",
        "\r\n",
        "  #helper function to build possible actions of the agent based on the problem statement\r\n",
        "  def actions(self):\r\n",
        "    possible_actions = [-1, 0, 1] # -1:decrease velocity by one, 0: keep velocity unchanged, 1: increase velocity by 1\r\n",
        "    return (list(itertools.product(possible_actions, possible_actions)))  \r\n",
        "    \r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnnHWYEt3Fwm"
      },
      "source": [
        "#Function to determine if a move of the agent from state s to state s_new causes a collision. \r\n",
        "def move_hit_obstacle(env, s, s_new):\r\n",
        "  x_a, y_a = s\r\n",
        "  x_b, y_b = s_new\r\n",
        "\r\n",
        "  if x_a > x_b:\r\n",
        "    min_x, max_x = x_b, x_a\r\n",
        "  else:\r\n",
        "    min_x, max_x = x_a, x_b\r\n",
        "\r\n",
        "  if y_a > y_b: \r\n",
        "    min_y, max_y = y_b, y_a\r\n",
        "  else:\r\n",
        "    min_y, max_y = y_a, y_b\r\n",
        "\r\n",
        "  dx = (max_x - min_x) + 1\r\n",
        "  dy = (max_y - min_y) + 1\r\n",
        "  t = env.arr[min_x:(min_x + dx),min_y:(min_y+dy)]\r\n",
        "  \r\n",
        "  for elem in t.flatten():\r\n",
        "    if elem==2:\r\n",
        "      return True\r\n",
        "  return False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDEPqfcEev65"
      },
      "source": [
        "last_ep = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hve1G6yPQhLS"
      },
      "source": [
        "#Function to implement the Q-Learning algorithm\r\n",
        "def Q_L(agent, env, alpha = 0.5,discount_factor = 0.9, epsilon = 0.2, episodes = 50000, goals_desired = 10000):\r\n",
        "  global last_ep\r\n",
        "  \r\n",
        "  #Initialize Q(S,A)\r\n",
        "  states = list(itertools.product(range(env.m), range(env.n)))\r\n",
        "  agent.initialize(states,env)\r\n",
        "\r\n",
        "  episode_number = 0\r\n",
        "  goals_crossed = 0\r\n",
        "  \r\n",
        "\r\n",
        "  for episode_number in tqdm(range(episodes)):\r\n",
        "\r\n",
        "    start_state = env.sample_start_state()\r\n",
        "    \r\n",
        "    s = start_state\r\n",
        "    a = agent.get_action(s)\r\n",
        "    agent.update_velocity(a)\r\n",
        "    r = env.get_reward(s)\r\n",
        "\r\n",
        "    episode = [(s,a,r)]\r\n",
        "    \r\n",
        "    s_new = env.update_state(s, agent.velocity)\r\n",
        "    hit_obstacle = move_hit_obstacle(env, s,s_new)\r\n",
        "\r\n",
        "    while (not env.isTerminal(s_new)) and (not hit_obstacle):\r\n",
        "      \r\n",
        "      a_star = agent.get_a_star(s_new)\r\n",
        "      agent.q[(s,a)] = agent.q[(s,a)] + alpha*(r + (discount_factor* agent.q[(s_new, a_star)]) - agent.q[(s,a)])\r\n",
        "\r\n",
        "\r\n",
        "      r = env.get_reward(s_new)\r\n",
        "      s = s_new\r\n",
        "      a = agent.get_action(s)\r\n",
        "      agent.update_velocity(a)\r\n",
        "\r\n",
        "      s_new = env.update_state(s, agent.velocity)\r\n",
        "      hit_obstacle = move_hit_obstacle(env,s,s_new)\r\n",
        "      episode.append((s,a,r))\r\n",
        "    \r\n",
        "    #Final Terminal state reward\r\n",
        "    \r\n",
        "    agent.q[(s,a)] = agent.q[(s,a)] + alpha*(r + (discount_factor * env.get_reward(s_new,hit_obstacle)) - agent.q[(s,a)])     \r\n",
        "\r\n",
        "    agent.reset_car()\r\n",
        "    \r\n",
        "    if hit_obstacle==False:\r\n",
        "      if s_new[0]>=env.m:        #acceleration caused car to go out of track crossing the finish line meanwhile\r\n",
        "        goals_crossed+=1\r\n",
        "        last_ep = (episode,s_new)\r\n",
        "      \r\n",
        "      if (0<=s_new[0]<env.m) and (0<=s_new[1]<env.n):\r\n",
        "        if env.arr[s_new[0], s_new[1]] == 3:\r\n",
        "          goals_crossed +=1\r\n",
        "          last_ep = (episode,s_new)\r\n",
        "\r\n",
        "    \r\n",
        "    if goals_crossed >=goals_desired:\r\n",
        "      print (\"Goals crossed surpassed goals desired to stop. Stopping Learning!\")\r\n",
        "      #last_ep = (episode,s_new)\r\n",
        "      break\r\n",
        "    \r\n",
        "  return goals_crossed"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6MTAkElfE-I"
      },
      "source": [
        "**Track 1**\r\n",
        "\r\n",
        "Building the environment for track 1 and applying the Q learning algorithm for the agent on this environment.\r\n",
        "\r\n",
        "Note: I took a steeper reward of 30 and penalty 30 for track 1 as a reward of 10 and penalty of 10 was causing the algorithm to take a lot more time to converge to 10K episodes where agent crossed goal state. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLMrXpz7i1J3",
        "outputId": "4fc62048-23ad-4254-87f6-0f262881f2d1"
      },
      "source": [
        "env = Environment('track-1.txt', reward = 30)\r\n",
        "td_agent = TD_Agent()\r\n",
        "Q_L(td_agent, env, alpha = 0.6, epsilon = 0.2, discount_factor = 0.95, episodes = 5000000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 2974296/5000000 [14:44<08:36, 3924.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Goals crossed surpassed goals desired to stop. Stopping Learning!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd8kWF2Yfebf"
      },
      "source": [
        "We can see that the agent crossed the goal state 10K times after about 2.9M episode. So, we stop the algorithm there and plot an episode of the agent to see how it learnt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "3Ue6q7Uc7Ltn",
        "outputId": "641630e8-e86c-4344-84a6-fcd16cb1389f"
      },
      "source": [
        "episode, end_state = last_ep\r\n",
        "end_reward = env.get_reward(end_state)\r\n",
        "env.plot_episode(episode, end_state)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAADnCAYAAABWvGk6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADTUlEQVR4nO3d0WqkQBBA0ell/zvmyzvvu1HwYtSJ57xKmGaYS0Eo7DHnfAH7/Ln6APCOhAOBcCAQDgTCgeDv1sOxjHv+y+0zHOtjHH8OfrW5zNUfjYkDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFgc8lzGctJx9hp+Sx/dPQpeDATBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDweaS510ty0f4m7IYCt8zcSAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgeBNd9X2750ty3L8QfjPU75nEwcC4UAgHAiEA4FwIBAOBMKBQDgQCAcC4UAgHAiEA8Etljz3vmDQkidXM3EgEA4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBDcYslzL7dOczUTBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDwS2WPO+6gGmZlDUmDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHgjHnXH84xvrDBzjrivczrqsvnn7F/ZxzrD0zcSAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EGy+yfPpS35nOWNps72V9Phz/BYmDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4ENzi1ml+ntuwj2XiQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAgsOTJodqLD99vAdXEgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAuFA8JglTzdon+MpX7OJA4FwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBwJInBCYOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQCAeCxyx5cl97r3i/w/XuJg4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwJLnlzuDkube5k4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAuFAYFeNy3khITyEcCAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgWDMOTeejo2HcIzx2vczm6/xQyf594Pm6geZOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCLaXPIFvmTgQCAcC4UAgHAiEA4FwIPgC7kROY+0aMlgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCv8uVo4f9__"
      },
      "source": [
        "From the plot, we can clearly see that the agent is successfully navigating itself across the racetrack by avoiding obstacles while maintaining the system and agent constraints. Here, **red = Start, Black = Obstacle, Green = Finish, Blue = Car trajectory**. I am also showing the trace of the car along the whole episode below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJWpqbScGvix"
      },
      "source": [
        "\r\n",
        "def show_trace(episode, end_state, end_reward):\r\n",
        "  trace = {'Time': [], 'State': [], 'Action': [], 'Reward':[]}\r\n",
        "  for i, ep in enumerate(episode):\r\n",
        "    s, a, r = ep\r\n",
        "    trace['Time'].append(i+1)\r\n",
        "    trace['State'].append(s)\r\n",
        "    trace['Action'].append(a)\r\n",
        "    trace['Reward'].append(r)\r\n",
        "\r\n",
        "  trace['Time'].append(len(episode)+1)\r\n",
        "  trace['State'].append(end_state)\r\n",
        "  trace['Action'].append(\"X\")\r\n",
        "  trace['Reward'].append(end_reward)\r\n",
        "\r\n",
        "  result = pd.DataFrame(trace)\r\n",
        "  print (result)\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPK3WWeqHz-b",
        "outputId": "09e6eddd-0be6-433e-f1be-56d8db7424b8"
      },
      "source": [
        "show_trace(episode, end_state, end_reward)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Time    State    Action  Reward\n",
            "0      1  (0, 12)    (1, 0)      -1\n",
            "1      2  (1, 12)    (1, 0)      -1\n",
            "2      3  (3, 12)   (1, -1)      -1\n",
            "3      4  (6, 11)  (-1, -1)      -1\n",
            "4      5   (8, 9)   (-1, 0)      -1\n",
            "5      6   (9, 7)    (0, 0)      -1\n",
            "6      7  (10, 5)    (0, 0)      -1\n",
            "7      8  (11, 3)   (-1, 1)      -1\n",
            "8      9  (11, 2)    (1, 1)      -1\n",
            "9     10  (12, 2)    (1, 1)      -1\n",
            "10    11  (14, 3)   (1, -1)      -1\n",
            "11    12  (17, 3)         X      30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4DN6590jaJv"
      },
      "source": [
        "From above we can see the trace of the car with the actions and rewards across the time steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OCOWBu8jiTO"
      },
      "source": [
        "**Track 2**\r\n",
        "\r\n",
        "Now, we build the environment as the environment of track 2 and run our algorithm for a new agent to learn from this environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KYGeq3BdHpq",
        "outputId": "e34f9c48-8ab8-4ab5-d42f-2e13b3d11df9"
      },
      "source": [
        "env = Environment('track-2.txt', reward = 50)\r\n",
        "td_agent = TD_Agent()\r\n",
        "Q_L(td_agent, env, alpha = 0.6, epsilon = 0.3, discount_factor = 0.95, episodes = 50000000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 9791817/50000000 [49:35<3:51:49, 2890.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Goals crossed surpassed goals desired to stop. Stopping Learning!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItxSPaxYj3jf"
      },
      "source": [
        "We can see that after around 9.7M episodes, the agent finally converged having crossed the goal state 10K times. We stop the algorithm here.\r\n",
        "\r\n",
        "Note: I took a further higher reward and penalty of 50 for this track. Taking less reward value would take more time. \r\n",
        "\r\n",
        "Below, the plot shows the agent navigating around the race track 2 while maintaining the action constraints. The trace for the same is also shown:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcsCM09mD4RR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "ea8d05e8-5653-4822-c638-e9a06afe8986"
      },
      "source": [
        "episode, end_state = last_ep\r\n",
        "end_reward = env.get_reward(end_state)\r\n",
        "env.plot_episode(episode, end_state)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAADnCAYAAACjQuKKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADMUlEQVR4nO3dwW6bQBRAUVP1vz3+8ukid1MpRcXBMZBzlnWtDIGrkaInZplz3oDb7de7FwBHIQaIGCBigIgB8nvtw2Us/tTE3x5PPBL3Zf91fMEc89MF2RkgYoCIASIGiBggYoCIASIGiBggYoCIAbI6mzSW8U3L4DTG45kv7b2Kl7AzQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNk/Uy3Mb5pGZzH3PyN8dQ5cN/PzgARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFADOr9cFvv8VmG7p5hZ4CIASIGiBggYoCIASIGiBggYoCIASIGyOps0quNcf+Gn3HdWRr2ZWeAiAEiBogYIGKAiAEiBogYIGKAiAEiBsius0lbZ42OODd0hWs4mrP8Tu0MEDFAxAARA0QMEDFAxAARA0QMEDFAxADZeTbp/HM6r76Gs8zp7Oks12BngIgBIgaIGCBigIgBIgaIGCBigIgBIgbI5d+bNMZ46f/f6nizT69Zx1e86zmyM0DEABEDRAwQMUDEABEDRAwQMUDEABEDxHuTLuYK98CZbvBmYoCIASIGiBggYoCIASIGiBggYoCIAbLrbBLXt/WdRh/f2TZr5L1J8GZigIgBIgaIGCBigIgBIgaIGCBigIgB8tbZpCOeAce6Z+7BM/NM72BngIgBIgaIGCBigIgBIgaIGCBigIgBIgbIMuf854ePx+PfH57EGOPdS+Bg5pzLZ/9uZ4CIASIGiBggYoCIASIGiBggYoCIASIGiBggl3+JmEE9/pedASIGiBggYoCIASIGiBggYoCIASIGiBggu84m/cQDC492zUdbz5nYGSBigIgBIgaIGCBigIgBIgaIGCBigIgBsnrA4bLcNh1weLS5m4/vmL3Z0xXuwf1+d8AhrBEDRAwQMUDEABEDRAwQMUDEABEDRAyQXWeTtpq3T0dEuJitj9HLn4s5zSbBGjFAxAARA0QMEDFAxAARA0QMEDFAxABZnU2Cn8TOABEDRAwQMUDEABED5A9W8XdeTnYGrAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lcneSw8ULPg",
        "outputId": "762d9d4e-9940-422b-b2b9-5e9df9fa1ea6"
      },
      "source": [
        "show_trace(episode, end_state, end_reward)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Time     State    Action  Reward\n",
            "0      1   (0, 12)   (1, -1)      -1\n",
            "1      2   (1, 11)   (0, -1)      -1\n",
            "2      3    (2, 9)  (-1, -1)      -1\n",
            "3      4    (2, 6)   (-1, 0)      -1\n",
            "4      5    (1, 3)    (0, 1)      -1\n",
            "5      6    (0, 1)    (1, 1)      -1\n",
            "6      7    (0, 0)    (1, 1)      -1\n",
            "7      8    (1, 0)    (0, 1)      -1\n",
            "8      9    (2, 1)    (0, 0)      -1\n",
            "9     10    (3, 2)   (1, -1)      -1\n",
            "10    11    (5, 2)   (1, -1)      -1\n",
            "11    12    (8, 1)   (-1, 1)      -1\n",
            "12    13   (10, 1)   (-1, 1)      -1\n",
            "13    14   (11, 2)   (-1, 0)      -1\n",
            "14    15   (11, 3)   (-1, 1)      -1\n",
            "15    16   (10, 5)    (0, 0)      -1\n",
            "16    17    (9, 7)    (0, 0)      -1\n",
            "17    18    (8, 9)   (-1, 0)      -1\n",
            "18    19   (6, 11)   (1, -1)      -1\n",
            "19    20   (5, 12)    (1, 0)      -1\n",
            "20    21   (5, 13)   (1, -1)      -1\n",
            "21    22   (6, 13)    (1, 0)      -1\n",
            "22    23   (8, 13)   (0, -1)      -1\n",
            "23    24  (10, 12)    (1, 0)      -1\n",
            "24    25  (13, 11)    (1, 1)      -1\n",
            "25    26  (17, 11)         X      50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWfDmn27kllX"
      },
      "source": [
        "**Track 3**\r\n",
        "\r\n",
        "This track is a custom and shorter track built to test out the algorithm for the problem statement.\r\n",
        "\r\n",
        "As the track is relatively simpler, we keep the reward and penalty value as 10 and increase the number of times required to cross goal state to 20K. The algorithm converges suitably after about 0.4M episodes. The plot of the car navigating the track and it's corressponding trace is shown below: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCAb4tqObYDV",
        "outputId": "e3369fd1-8682-4d30-9f2f-b62ed4947bf7"
      },
      "source": [
        "td_agent = TD_Agent()\r\n",
        "env = Environment('test_track.txt', reward = 10)\r\n",
        "Q_L(td_agent, env, alpha = 0.5, episodes = 5000000, goals_desired = 20000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9%|▊         | 432884/5000000 [01:29<12:47, 5954.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Goals crossed surpassed goals desired to stop. Stopping Learning!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ToH8yMLhpkgB",
        "outputId": "6f3e0450-4c38-454f-861e-11ad81660146"
      },
      "source": [
        "episode, end_state = last_ep\r\n",
        "end_reward = env.get_reward(end_state)\r\n",
        "env.plot_episode(episode, end_state)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADjCAYAAAAmP8cGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADu0lEQVR4nO3aMa7bMBBAwc/A9zZ18k2TVgZS5DGwZtotlpaEBxZeM/MDQOPX6QMAPInoAoREFyAkugAh0QUIiS5A6PVpuPbyfzK+03Xo036vM3tJzZ7bF+2mCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAoTUz98O17odfau99+gh8sb3fB7evg7ufZWZuH7abLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChF6fhnvv6Bh41s9w9jUfXc4fbroAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYDQmpnb4XVd90Pgr+39Prj7OrR3H9l70sysu5mbLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChF6nD/C/2XufPkLumb/5fWjvdWTvSU/8vj5x0wUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoRepw/wv9l7nz7CY+z9Prj7OrabZ3PTBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2A0JqZD9P1Yci3WD9nXvP8rCN74Z+buf243XQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugChNTOnzwDwGG66ACHRBQiJLkBIdAFCogsQEl2A0G/LsD07mRb+kgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyhJroHj8_BT",
        "outputId": "e87edc11-9091-4e5c-9245-94c1c6afc66f"
      },
      "source": [
        "show_trace(episode, end_state, end_reward)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Time   State   Action  Reward\n",
            "0     1  (0, 4)   (1, 1)      -1\n",
            "1     2  (1, 5)   (0, 0)      -1\n",
            "2     3  (2, 6)   (0, 0)      -1\n",
            "3     4  (3, 7)  (1, -1)      -1\n",
            "4     5  (5, 7)  (1, -1)      -1\n",
            "5     6  (8, 6)        X      10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YRcAdu8Uz9Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}